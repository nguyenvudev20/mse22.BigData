{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# I. (30 marks) Below are the theoretical session, which requires you to write down in thw cell \"Text\" in the notebook:\n",
        "1. Could you list out the main challenges of concurrency?\n",
        "2. Could you describe shortly about MapReduce? Please provide an example of MapReduce.\n",
        "3. Provide a high level comparison of Apache Hadoop and Apache Spark.\n",
        "4. What are the advantages of Apache Spark?\n",
        "5. Provide a comparison of RDD and DataFrame in Spark.  "
      ],
      "metadata": {
        "id": "V-S6ggQMmxvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 -  Could you list out the main challenges of concurrency?\n",
        "\n",
        "Concurrency refers to the ability of a system to handle multiple tasks simultaneously. Some of the main challenges of concurrency include:\n",
        "  * Race conditions: When multiple threads or processes access shared resources, the order of execution can lead to unexpected results.\n",
        "  * Deadlocks: Two or more processes are unable to proceed because each is waiting for the other to release a resource.\n",
        "  * Synchronization: Coordinating access to shared resources to avoid conflicts and maintain data consistency.\n",
        "  * Performance overhead: Managing and switching between threads or processes can lead to overhead, reducing overall system performance.\n",
        "  * Resource management: Allocating and managing resources efficiently among concurrent processes.\n",
        "\n"
      ],
      "metadata": {
        "id": "9nQ3TH20nm6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 - Could you describe shortly about MapReduce? Please provide an example of MapReduce.\n",
        "\n",
        "MapReduce is a programming model and processing framework designed for parallel processing of large-scale data. It was popularized by Google and later adopted by Apache Hadoop as a fundamental component of its data processing ecosystem.\n",
        "  In MapReduce, data processing tasks are divided into two phases:\n",
        "\n",
        "  * Map phase: The input data is split into smaller chunks, and a map function is applied to each chunk independently, producing intermediate key-value pairs.\n",
        "  * Reduce phase: The intermediate results are shuffled and sorted based on the keys and then processed by a reduce function, which aggregates the data and produces the final output.\n",
        "\n",
        "  Example of MapReduce:\n",
        "  Let's say we have a large collection of documents and want to count the occurrences of each word. In MapReduce, we would:\n",
        "\n",
        "  * Map phase: For each document, the map function emits (word, 1) key-value pairs, where the word is the key, and the value is set to 1.\n",
        "  * Reduce phase: The reduce function receives all the (word, 1) pairs and sums up the values to get the total count for each word."
      ],
      "metadata": {
        "id": "ZGtSOV1Wr2oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3 - Provide a high level comparison of Apache Hadoop and Apache Spark.\n",
        "\n",
        "\n",
        "| Feature                  | Apache Hadoop                                   | Apache Spark                                       |\n",
        "|--------------------------|-------------------------------------------------|----------------------------------------------------|\n",
        "| Processing Model        | Primarily MapReduce-based for batch processing  | Offers batch processing, interactive queries, streaming, and machine learning  |\n",
        "| Speed                    | Slower due to disk-based processing            | Faster due to in-memory processing                |\n",
        "| Ease of Use              | Lower-level MapReduce API                       | Higher-level APIs (Java, Scala, Python, SQL)       |\n",
        "| Memory Management       | Relies on disk storage                          | In-memory caching capabilities                     |\n",
        "| Fault Tolerance         | Provides fault tolerance through replication and HDFS | Provides fault tolerance through lineage information  |\n",
        "| Libraries                 | External libraries for machine learning, graph processing, etc. | Built-in libraries for machine learning (MLlib), graph processing (GraphX), and streaming (Spark Streaming)  |\n",
        "| Versatility               | Primarily used for batch processing             |"
      ],
      "metadata": {
        "id": "ZRbn42eHr7mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 -  What are the advantages of Apache Spark?\n",
        "Apache Spark offers several advantages over traditional data processing frameworks like Hadoop's MapReduce:\n",
        "* Speed: Spark's in-memory processing allows it to perform tasks much faster than MapReduce, particularly for iterative algorithms and interactive queries.\n",
        "* Ease of use: Spark provides higher-level APIs in multiple languages, making it more user-friendly and reducing the amount of boilerplate code required.\n",
        "* Versatility: Spark supports batch processing, interactive queries, real-time streaming, and machine learning, making it a versatile choice for various data processing tasks.\n",
        "* Fault tolerance: Spark automatically recovers from failures and maintains data consistency using lineage information, similar to Hadoop's MapReduce.\n",
        "* Rich ecosystem: Spark comes with libraries for machine learning, graph processing, and stream processing, expanding its capabilities beyond basic data processing.\n"
      ],
      "metadata": {
        "id": "oWesFEp1sAA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 5 -  Provide a comparison of RDD and DataFrame in Spark.\n",
        "\n",
        "\n",
        "\n",
        "| Feature                    | RDD (Resilient Distributed Dataset)      | DataFrame                                             |\n",
        "|----------------------------|---------------------------------------|-------------------------------------------------------|\n",
        "| Data Abstraction           | Low-level distributed collection of objects   | Higher-level distributed collection of structured data |\n",
        "| API                         | Functional API with transformations and actions  | SQL-like API with DataFrame operations                 |\n",
        "| Optimization               | No built-in optimization                 | Catalyst query optimizer and Tungsten execution engine |\n",
        "| Schema                     | No predefined schema                      | Structured data with a defined schema                  |\n",
        "| Language Support        | Java, Scala, Python, and more         | Java, Scala, Python, and SQL                               |\n",
        "| Fault Tolerance            | Uses lineage information for fault tolerance   | Uses lineage information for fault tolerance              |\n",
        "| Performance                | May require manual optimization for performance | Optimized for better performance out-of-the-box          |\n",
        "| Usage                       | Suitable for unstructured and semi-structured data | Suitable for structured and semi-structured data      |\n",
        "\n",
        "- Both RDDs and DataFrames are essential components of Apache Spark, and the choice between them depends on the nature of the data and the operations you need to perform. RDDs are more versatile and can handle unstructured data, but require more manual optimizations. On the other hand, DataFrames offer better performance, support structured data with a predefined schema, and provide a more user-friendly API with SQL-like queries."
      ],
      "metadata": {
        "id": "6qIfzArusIJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. (30 marks) You are given a file `appl_stock.csv`, please carry out the following tasks:\n",
        "\n",
        "1. Read this file by PySpark. Print out the schema.\n",
        "2. Create columns of `day of month`, `hour`, `day of year`, `month` from the column `Date` of the data.\n",
        "3. Using `groupby` and `year()` function to compute the average closing price per year."
      ],
      "metadata": {
        "id": "S9_563ulpsh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "mONn36_Yc8Dm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://media.githubusercontent.com/media/nguyenvudev20/mse22.BigData/main/Final_exam/appl_stock.csv'"
      ],
      "metadata": {
        "id": "h_q690ZRq5kV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff37607-c9eb-4964-a8a3-0c5413c6a11d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 07:24:40--  https://media.githubusercontent.com/media/nguyenvudev20/mse22.BigData/main/Final_exam/appl_stock.csv\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 143130 (140K) [text/plain]\n",
            "Saving to: ‘appl_stock.csv’\n",
            "\n",
            "\rappl_stock.csv        0%[                    ]       0  --.-KB/s               \rappl_stock.csv      100%[===================>] 139.78K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 07:24:40 (6.04 MB/s) - ‘appl_stock.csv’ saved [143130/143130]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(master = 'local')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "          .appName(\"Python Spark SQL Final Exam\") \\\n",
        "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "          .getOrCreate()"
      ],
      "metadata": {
        "id": "wL-UKvumfLtJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1 - Read this file by PySpark. Print out the schema.\n"
      ],
      "metadata": {
        "id": "MFvughs8sdpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## II 1.Read this file by PySpark. Print out the schema.\n",
        "ad = spark.read.csv('appl_stock.csv', header=True, inferSchema=True)\n",
        "ad.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWdLxXV8fYoC",
        "outputId": "4cd6870a-064d-4e27-953d-8b59d028476a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
            "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
            "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
            "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
            "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
            "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
            "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ad.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO0wxtFzf3Gg",
        "outputId": "e24ddd0b-c278-4c07-fe95-9d28a4e6f364"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 2 - Create columns of `day of month`, `hour`, `day of year`, `month` from the column `Date` of the data.\n"
      ],
      "metadata": {
        "id": "VsWKNQY8shQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Create columns of day of month, hour, day of year, month from the column Date of the data.\n",
        "\n",
        "from pyspark.sql.functions import col, dayofmonth, month, year,hour,dayofyear\n",
        "ad = ad.withColumn(\"day of month\", dayofmonth(col(\"Date\")))\n",
        "ad = ad.withColumn(\"month \", month(col(\"Date\")))\n",
        "ad = ad.withColumn(\"hour\", hour(col(\"Date\")))\n",
        "ad = ad.withColumn(\"day of year\", dayofyear(col(\"Date\")))\n",
        "ad.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MX81rIRgg0r",
        "outputId": "d247f112-c7fd-49e1-f8ad-51d43d459c15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+------------------+---------+------------------+------------+------+----+-----------+\n",
            "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|day of month|month |hour|day of year|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+------------+------+----+-----------+\n",
            "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|           4|     1|   0|          4|\n",
            "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|           5|     1|   0|          5|\n",
            "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|           6|     1|   0|          6|\n",
            "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|           7|     1|   0|          7|\n",
            "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|           8|     1|   0|          8|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+------------+------+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 - Using `groupby` and `year()` function to compute the average closing price per year."
      ],
      "metadata": {
        "id": "L2tEPOC8sk9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Using groupby and year() function to compute the average closing price per year.\n",
        "ad = ad.withColumn(\"year\", year(col(\"Date\")))\n",
        "average_closing_per_year = ad.groupBy(\"year\").avg(\"Close\").alias(\"average_closing\").orderBy(\"Year\")\n",
        "average_closing_per_year.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ-h8IdThiWW",
        "outputId": "3e675344-d2a1-453e-a491-4345793baabc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|year|        avg(Close)|\n",
            "+----+------------------+\n",
            "|2010| 259.8424600000002|\n",
            "|2011|364.00432532142867|\n",
            "|2012| 576.0497195640002|\n",
            "|2013| 472.6348802857143|\n",
            "|2014| 295.4023416507935|\n",
            "|2015|120.03999980555547|\n",
            "|2016|104.60400786904763|\n",
            "+----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. (40 marks) You are given a data `customer_churn.csv`, which describes the churn status in clients of a marletting agency. As a data scientist, you are required to create a machine learning model **in Spark** that will help predict which customers will churn (stop buying their service). A short description of the data is as follow:\n",
        "```\n",
        "Name : Name of the latest contact at Company\n",
        "Age: Customer Age\n",
        "Total_Purchase: Total Ads Purchased\n",
        "Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
        "Years: Totaly Years as a customer\n",
        "Num_sites: Number of websites that use the service.\n",
        "Onboard_date: Date that the name of the latest contact was onboarded\n",
        "Location: Client HQ Address\n",
        "Company: Name of Client Company\n",
        "```\n",
        "\n",
        "1. Read, print the schema and check out the data to set the first sight of the data.\n",
        "2. Format the data according to `VectorAssembler`, which is supported in MLlib of PySpark.\n",
        "3. Split the data into train/test data, and then fit train data to the logistic regression model.\n",
        "4. Evaluate the results and compute the AUC."
      ],
      "metadata": {
        "id": "brQ8gRaUshXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 - Read, print the schema and check out the data to set the first sight of the"
      ],
      "metadata": {
        "id": "M4AgVxfRspv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Read, print the schema and check out the data to set the first sight of the data.\n",
        "!wget 'https://media.githubusercontent.com/media/nguyenvudev20/mse22.BigData/main/Final_exam/customer_churn.csv'\n",
        "ad2 = spark.read.csv('customer_churn.csv', header=True, inferSchema=True)\n",
        "ad2.show(5)"
      ],
      "metadata": {
        "id": "fN4Zb88PtzCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7272d94f-1754-4cb8-e1f5-0e801753edc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 07:41:20--  https://media.githubusercontent.com/media/nguyenvudev20/mse22.BigData/main/Final_exam/customer_churn.csv\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115479 (113K) [text/plain]\n",
            "Saving to: ‘customer_churn.csv’\n",
            "\n",
            "customer_churn.csv  100%[===================>] 112.77K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 07:41:20 (5.52 MB/s) - ‘customer_churn.csv’ saved [115479/115479]\n",
            "\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
            "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
            "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
            "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
            "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ad2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hyDFQx2jIR7",
        "outputId": "0d5c56dc-e757-43cb-cbf0-354313169581"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 - Format the data according to `VectorAssembler`, which is supported in MLlib of PySpark.\n"
      ],
      "metadata": {
        "id": "dwg2xAIust5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Format the data according to VectorAssembler, which is supported in MLlib of PySpark.\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "feature_columns = [\"Age\", \"Total_Purchase\", \"Account_Manager\", \"Years\", \"Num_Sites\"]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "ad2 = assembler.transform(ad2)\n"
      ],
      "metadata": {
        "id": "BPNXPTaCjvwv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ad2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etgv7zxBkck2",
        "outputId": "c5de024d-9db1-4c0b-9da6-8dacd8c88109"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
            "|              Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|            features|\n",
            "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
            "|   Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|[42.0,11066.8,0.0...|\n",
            "|      Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|[41.0,11916.22,0....|\n",
            "|        Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|[38.0,12884.75,0....|\n",
            "|      Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|[42.0,8010.76,0.0...|\n",
            "|     Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|[37.0,9191.58,0.0...|\n",
            "|   Jessica Williams|48.0|      10356.02|              0| 5.12|      8.0|2009-03-03 23:13:37|6187 Olson Mounta...|        Kelly-Warren|    1|[48.0,10356.02,0....|\n",
            "|        Eric Butler|44.0|      11331.58|              1| 5.23|     11.0|2016-12-05 03:35:43|4846 Savannah Roa...|   Reynolds-Sheppard|    1|[44.0,11331.58,1....|\n",
            "|      Zachary Walsh|32.0|       9885.12|              1| 6.92|      9.0|2006-03-09 14:50:20|25271 Roy Express...|          Singh-Cole|    1|[32.0,9885.12,1.0...|\n",
            "|        Ashlee Carr|43.0|       14062.6|              1| 5.46|     11.0|2011-09-29 05:47:23|3725 Caroline Str...|           Lopez PLC|    1|[43.0,14062.6,1.0...|\n",
            "|     Jennifer Lynch|40.0|       8066.94|              1| 7.11|     11.0|2006-03-28 15:42:45|363 Sandra Lodge ...|       Reed-Martinez|    1|[40.0,8066.94,1.0...|\n",
            "|       Paula Harris|30.0|      11575.37|              1| 5.22|      8.0|2016-11-13 13:13:01|Unit 8120 Box 916...|Briggs, Lamb and ...|    1|[30.0,11575.37,1....|\n",
            "|     Bruce Phillips|45.0|       8771.02|              1| 6.64|     11.0|2015-05-28 12:14:03|Unit 1895 Box 094...|    Figueroa-Maynard|    1|[45.0,8771.02,1.0...|\n",
            "|       Craig Garner|45.0|       8988.67|              1| 4.84|     11.0|2011-02-16 08:10:47|897 Kelley Overpa...|     Abbott-Thompson|    1|[45.0,8988.67,1.0...|\n",
            "|       Nicole Olson|40.0|       8283.32|              1|  5.1|     13.0|2012-11-22 05:35:03|11488 Weaver Cape...|Smith, Kim and Ma...|    1|[40.0,8283.32,1.0...|\n",
            "|     Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|2015-03-28 02:13:44|1774 Peter Row Ap...|Snyder, Lee and M...|    1|[41.0,6569.87,1.0...|\n",
            "|       James Wright|38.0|      10494.82|              1| 6.81|     12.0|2015-07-22 08:38:40|45408 David Path ...|      Sanders-Pierce|    1|[38.0,10494.82,1....|\n",
            "|      Doris Wilkins|45.0|       8213.41|              1| 7.35|     11.0|2006-09-03 06:13:55|28216 Wright Moun...|Andrews, Adams an...|    1|[45.0,8213.41,1.0...|\n",
            "|Katherine Carpenter|43.0|      11226.88|              0| 8.08|     12.0|2006-10-22 04:42:38|Unit 4948 Box 481...|Morgan, Phillips ...|    1|[43.0,11226.88,0....|\n",
            "|     Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|2015-10-07 00:27:10|69203 Crosby Divi...|      Villanueva LLC|    1|[53.0,5515.09,0.0...|\n",
            "|        Kathy Curry|46.0|        8046.4|              1| 5.69|      8.0|2014-11-06 23:47:14|9569 Caldwell Cre...|Berry, Orr and Ca...|    1|[46.0,8046.4,1.0,...|\n",
            "+-------------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3 - Split the data into train/test data, and then fit train data to the logistic regression model.\n"
      ],
      "metadata": {
        "id": "NOeeggw2sxqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Split the data into train/test data, and then fit train data to the logistic regression model.\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "(train_data, test_data) = ad2.randomSplit([0.8, 0.2], seed=42)\n",
        "lr_model = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\")\n",
        "lr_model = lr_model.fit(train_data)"
      ],
      "metadata": {
        "id": "XAUpdrvlkm20"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lr_model.transform(test_data)\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnM4GTjIk5nV",
        "outputId": "6e1702e5-2ef0-410e-bfa4-1466258903a9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|            Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|            features|       rawPrediction|         probability|prediction|\n",
            "+-----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|       Aaron West|55.0|      10056.55|              0| 4.98|      8.0|2006-09-01 06:11:47|071 Schmidt Locks...|Cruz, Russell and...|    0|[55.0,10056.55,0....|[3.20676315272588...|[0.96108799487693...|       0.0|\n",
            "|      Adam Harris|44.0|       9815.03|              1|  4.9|      9.0|2016-05-29 06:00:09|40488 Michael For...|Smith, Oconnor an...|    0|[44.0,9815.03,1.0...|[2.14403994483814...|[0.89511051742186...|       0.0|\n",
            "|      Adam Porter|48.0|      10746.37|              1| 4.22|      8.0|2006-02-18 01:22:53|92600 Kevin Cape ...|        Anthony-Wood|    0|[48.0,10746.37,1....|[3.47336167625877...|[0.96992025036506...|       0.0|\n",
            "|   Albert Clayton|33.0|      12115.91|              1|  5.5|      9.0|2010-08-09 00:14:20|Unit 6274 Box 532...|Mann, Roberts and...|    0|[33.0,12115.91,1....|[2.43321779590050...|[0.91932550646215...|       0.0|\n",
            "|Alyssa Harper DDS|45.0|       9874.69|              1| 2.41|      9.0|2006-09-04 07:20:52|63593 Hector Miss...|     Garza-Alexander|    0|[45.0,9874.69,1.0...|[3.46249297743352...|[0.96960153183766...|       0.0|\n",
            "| Amanda Hernandez|38.0|      10298.29|              1| 5.07|      9.0|2015-05-31 21:15:18|4792 Amy Wall Apt...|Barry, Brown and ...|    0|[38.0,10298.29,1....|[2.42176328053249...|[0.91847187871890...|       0.0|\n",
            "|      Amber Evans|37.0|      10348.37|              1| 7.65|      6.0|2006-04-20 00:24:51|1753 John Lights ...|         Weiss-Kelly|    0|[37.0,10348.37,1....|[4.80344030046415...|[0.99186523424924...|       0.0|\n",
            "|       Amy Garcia|42.0|       6406.38|              0|  3.0|      9.0|2007-03-09 23:50:39|12223 Sean Valley...|          Barker Ltd|    0|[42.0,6406.38,0.0...|[4.06173988653070...|[0.98307244231677...|       0.0|\n",
            "|     Andrea Salas|42.0|      11473.38|              1| 2.87|     10.0|2015-03-19 22:32:48|308 Graham Corner...|       Blackwell PLC|    0|[42.0,11473.38,1....|[2.08393086432979...|[0.88933150529603...|       0.0|\n",
            "|    Andrea Warner|46.0|       10469.5|              0| 6.54|     10.0|2011-03-24 17:01:00|753 Hernandez Pin...|          Bailey LLC|    0|[46.0,10469.5,0.0...|[0.40660830314025...|[0.60027433541593...|       0.0|\n",
            "|     Andrew Gates|48.0|      12810.16|              0| 2.21|     10.0|2015-03-27 04:57:10|779 Jeremy Road P...|Conner, Brown and...|    0|[48.0,12810.16,0....|[2.58828185039909...|[0.93010360119452...|       0.0|\n",
            "|      Andrew Kent|45.0|      12374.97|              0| 4.66|      7.0|2011-12-24 17:00:01|79904 Jacobson We...|Briggs, Cross and...|    0|[45.0,12374.97,0....|[5.19362104622529...|[0.99447878623679...|       0.0|\n",
            "|    Andrew Moreno|43.0|      12068.39|              1| 3.59|      7.0|2008-03-28 21:32:43|60669 Rivera Ligh...|      Williams Group|    0|[43.0,12068.39,1....|[5.34745055285008...|[0.99526228122485...|       0.0|\n",
            "|      Ann Johnson|42.0|       9091.99|              0| 5.85|      6.0|2014-03-27 05:11:22|2091 Sims Ranch L...|          Walker Ltd|    0|[42.0,9091.99,0.0...|[6.11691755638645...|[0.99779960936153...|       0.0|\n",
            "|   Anthony Molina|26.0|       8939.61|              0| 4.54|      7.0|2007-09-22 09:46:47|255 Olson Mountai...|Brown, Johnson an...|    0|[26.0,8939.61,0.0...|[6.64824158810468...|[0.99870537868593...|       0.0|\n",
            "|      Ashley Soto|49.0|       9853.64|              1| 6.95|      6.0|2012-01-01 21:48:50|275 Thompson Stre...|           Brown Inc|    0|[49.0,9853.64,1.0...|[4.42861779031865...|[0.98820970032211...|       0.0|\n",
            "|  Barry Gallagher|52.0|       6805.48|              1| 6.05|      9.0|2010-06-04 12:28:32|35333 Carla Keys ...|          Butler PLC|    0|[52.0,6805.48,1.0...|[1.10659163418772...|[0.75149314079101...|       0.0|\n",
            "|     Brenda Haley|41.0|      14069.41|              1| 1.62|     10.0|2010-10-14 07:46:45|USS Barr FPO AE 0...|          Forbes LLC|    0|[41.0,14069.41,1....|[2.73630418672583...|[0.93913518547082...|       0.0|\n",
            "|      Brian Young|49.0|      10522.21|              0| 7.25|     14.0|2014-11-08 00:40:36|Unit 6711 Box 796...|           Munoz Ltd|    1|[49.0,10522.21,0....|[-5.1937348199307...|[0.00552058909765...|       1.0|\n",
            "| Brittany Jackson|50.0|       12682.9|              0| 4.58|     10.0|2006-09-11 07:07:17|082 Roth Island D...|     Mcdonald-Cooper|    1|[50.0,12682.9,0.0...|[1.14314371897740...|[0.75825636192958...|       0.0|\n",
            "+-----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 - Evaluate the results and compute the AUC."
      ],
      "metadata": {
        "id": "8pG9yHIQs2C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Evaluate the results and compute the AUC.\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxRHKslkmSEb",
        "outputId": "8a1feae3-8f62-4f5d-8d1d-cfd8e3985bfb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.8798426150121073\n"
          ]
        }
      ]
    }
  ]
}